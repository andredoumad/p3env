from alice.spiders.AliceRequiredModules import *
from alice.spiders.Alice import *
from alice.spiders.AliceROM import *
from alice.spiders.AliceSupport import *
from html_sanitizer import Sanitizer
from email_scraper import scrape_emails
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.loader import ItemLoader
from scrapy.selector import Selector

class AliceSpider(scrapy.Spider):
		def __init__(self, name):
    	name = "net"
		session = Session(webdriver_path=chromedriverFilePath, browser='chrome', default_timeout=15, webdriver_options={'arguments': ['headless']})
		options = Options()
		options.add_argument("--headless") # Runs Chrome in headless mode.
		options.add_argument('--no-sandbox') # # Bypass OS security model
		options.add_argument('start-maximized')
		options.add_argument('disable-infobars')
		options.add_argument("--disable-extensions")
		sourceCodeFilePath = str(str(getframeinfo(currentframe()).filename))

		start_urls = ["https://en.wikipedia.org/wiki/Health_care"]
		url = "https://en.wikipedia.org/wiki/Health_care"
		Crawling = True
		searchString = "https://en.wikipedia.org/wiki/Health_care"
		webCrawler = Prototype('webCrawler')
		webCrawler.alice.initialize()
		def parse(self, response):
			session.driver.get(searchString)
			raw =  session.driver.page_source
			textFP = str(os + '/' + 'text.html')
			emailFP =str(dataPath + '/' + 'emails.html')
			linksFP = str(dataPath + '/' + 'links.html')
			print(textFP)
			print(emailFP)
			print(linksFP)
			sleep(1)
			f = open(textFP, 'w+')
			f.write(raw)
			f.close()
			f = open(textFP, 'r')
			f.read(textFP)
			f.close()
			soup = BeautifulSoup(textFP)
			url_text = soup.get_text()
			emails = scrape_emails(url_text)
			f = open(emailFP, 'w+')
			f.write(emails)
			f.close()
			hrefs = soup.findall('href')
			f = open(linksFP, 'w+')
			f.write(hrefs)
			f.close()
			text = soup.get_text()
			sanitizer = Sanitizer({
				'tags': ('h1', 'h2', 'p'),
				'attributes': {},
				'empty': set(),
				'separate': set(),
			})
			sanitizer.sanitize(text)
			f = open(dataPath + "textFP.html", 'w+')
			f.write(text)
			f.close()
			webCrawler.alice.MineWebsite(domain, textFP, linksFP, emailFP)