

map the nearest link associated with the english.txt

navigation matrix:

Language_entity




































https://github.com/EmilStenstrom/conllu





clear_file
online_navigate_critical_access_hospitals
quit_session_get_new_google_search
click_first_google_result
bot_db_initialize_filepaths
selenium_download_page_source
getPathsAndTime
writeFileAs
getProperTime
|FileSystemIO|writeFileAs|  
getFilePathWithoutsuffix
getListWithoutDuplicates
get_list_ignore
getCleanurls
mapUniqueLinks
map_url_by_phrase
update
get_memory
Memory   <<<< we need to use the instance of memory, when it's created in order to do this work.













google search by ensure_element_by_css_selector
first result
#rso > div:nth-child(1) > div > div > div > div.r > a
#rso > div:nth-child(1) > div > div.kp-blk.c2xzTb.Wnoohf.OJXvsb > div > div.ifM9O > div:nth-child(3) > div.g > div > div > div.r > a
#rso > div:nth-child(2) > div > div:nth-child(1) > div > div > div.r > a


#rso > div:nth-child(2) > div > div > div > div.r > a

#rso > div:nth-child(3) > div > div:nth-child(1) > div > div > div.r > a

#rso > div:nth-child(4) > div > div:nth-child(1) > div > div > div.r > a

google search xpaths

first result
//*[@id="rso"]/div[1]/div/div/div/div/div[1]/a/h3
//*[@id="rso"]/div[1]/div/div[1]/div/div/div[1]/a
//*[@id="rso"]/div[1]/div/div/div/div[1]/a
//*[@id="rso"]/div[4]/div/div[1]/div/div/div[1]/a
//*[@id="rso"]/div[2]/div/div[1]/div/div/div[1]/a/h3
//*[@id="rso"]/div[4]/div/div[1]/div/div/div[1]/a/h3
//*[@id="rso"]/div[1]/div/div/div/div[1]/a/h3













TARGETS 

<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-has-children menu-item-557"><a rel="page" href="https://www.abbevilleareamc.com/abbeville-area-medical-center/about-us/" class="elementor-item has-submenu" id="sm-15609196164801418-1" aria-haspopup="true" aria-controls="sm-15609196164801418-2" aria-expanded="false">About Us<span class="sub-arrow"><i class="fa"></i></span></a>


<li class="menu-item menu-item-type-post_type menu-item-object-page menu-item-558"><a rel="page" href="https://www.abbevilleareamc.com/abbeville-area-medical-center/about-us/administration/" class="elementor-sub-item">Administration</a></li>



<link rel="stylesheet" href="//core.secure.ehc.com/src/apps/stacks/about-us/overlay/v1/about-us-overlay-v1.min.css" type="text/css" />




isolate the following:



content=" ... "
class="h1 ... "
<p> ... </p
span> ... <
title"> ... <

alt= ... <
<em> ... </em>
strong> ... <

br /> ... <


harvest is mismatched with urls





Geotext extracts country and city mentions from text 



+++set+++
consoleInterval
0.025

Main Menu:
    Command line interface. 



        #demo = displays a report generated by mining for "Andre Doumad"

        #help = displays the help menu

        #define = define xlsx workbook with fields and sheets

        #configure = takes you to another menu for adjusting anything

        mine = load target into the active matrix
            filename, filepath, domain

        #filename = this is only the file name with extension,
            #EXAMPLES: demo.csv test.xlsx, data.txt, sample.html

        #directorypath = 

        #domain = 

        #save

        #export

        #import

        #email

        #schedule

        #print = displays a preview of the referenced target

        #automate = Develop a Finite State Machine to automate work.
            #reports, tasks, schedules, targets, emails
            #states, sequences, actions, decisions, conditions
            #transitions, jobs

        #shutdown

        Example sequence:
        mine https://hallutkonvolut.wordpress.com/2016/06/05/the-joker-alan-watts/

---------------- ---------------- ---------------- ---------------- ----------------
state commands     actions         arguments          decisions
---------------- ---------------- ---------------- ---------------- ----------------














https://github.com/mherrmann/fbs
package standalone build with fman build system -- makes gui packaging a breeze

Task scheduling network scheduling -- install CELERY


		#alice remembers the following information
		#ram_www_history.csv
		#RowNumber,dateFirstVisited,vistDate,domain,lastDiscoveryCount,#totalDiscoveries,totalURLs,totalWords,totalPhrases,totalRequests,#totalFailedRequests,progress,ahapRanking,rankCertainty

		#alice discovers new web pages and prioritizes them
		#ram_www_future.csv
		#RowNumber,foundDate,initialDiscoveryDomain,uniqueReferenceCount,#referenceAhapRanking,priority

		#alice records every url from every domain
		#ram_url_links.csv
		#RowNumber,timestamp,domainSource,hyperlink,hyperlinkText,totalFrequency,#uniqueDomainFrequency,ahapKeys,aliceKeys,good,neutral,negative

		#alice records every english word from every domain
		#ram_nlp_words.csv
		#RowNumber,timestamp,domain,word,totalFrequency,uniqueDomainFrequency,#ahapMatches,good,neutral,negative

		#alice records every english phrase from every domain
		#ram_nlp_phrases.csv
		#RowNumber,timestamp,domain,phrase,totalFrequency,uniqueDomainFrequency,#ahapKeys,aliceKeys,good,neutral,negative

		#alice interprets natural language for each domain
		#RowNumber,domain,summary,classification

		#alice generates a report for ahap
		#ram_ahap_report.xlsx
		#RowNumber,timestamp,name,summary,ranking,domain,email,title,address,phone,#comment,frequency,recency

url_extractor (0.0.2)


pyaddr (1.0.2)                           - extract address from text


location-extractor (7.9)                 - Extract locations from text

email_extractor (v1)                     - Extract Emails by using webcrawl Extract Url links from a site

s.driver.ensure_element_by_xpath("//li[@class='b1']", state='clickable', timeout=5).ensure_click()


python -m textblob.download_corpora
python -m textblob.download_corpora
python -m textblob.download_corpora
python -m textblob.download_corpora
python -m textblob.download_corpora
python -m textblob.download_corpora






# === We also added these methods named in accordance to Selenium's api design ===
# ensure_element_by_id
# ensure_element_by_name
# ensure_element_by_link_text
# ensure_element_by_partial_link_text
# ensure_element_by_tag_name
# ensure_element_by_class_name
# ensure_element_by_css_selector





https://github.com/openeventdata/mordecai

https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md
add numpy for summary




https://nlpforhackers.io/libraries/
https://nlpforhackers.io/libraries/
https://nlpforhackers.io/libraries/
https://nlpforhackers.io/libraries/
https://nlpforhackers.io/libraries/
https://nlpforhackers.io/libraries/
https://nlpforhackers.io/libraries/
q



https://stackoverflow.com/questions/52083862/python-call-child-method-inside-parent-method-from-child-method


LEARN MORE ABOUT THE SUPER METHOD





New

'{:{align}{width}}'.format('test', align='^', width='10')

Output

   test   








New

'{:.{prec}} = {:.{prec}f}'.format('Gibberish', 2.7182, prec=3)

Output

Gib = 2.718



New

'{:{width}.{prec}f}'.format(2.7182, width=5, prec=2)

Output

 2.72





https://pyformat.info/

New

'{:>10}'.format('test')

Output

      test

https://pyformat.info/



New

'{:10}'.format('test')

Output

test 
https://pyformat.info/


New

'{:_<10}'.format('test')

Output

test______

https://pyformat.info/


New

'{:^10}'.format('test')

Output

   test   

https://pyformat.info/

New

'{:^6}'.format('zip')

Output

 zip  

https://pyformat.info/


New

'{:.5}'.format('xylophone')

Output

xylop




>>> "{0:<20s} {1:6.2f}".format('Spam & Eggs:', 6.99)
'Spam & Eggs:           6.99'
>>> "{0:>20s} {1:6.2f}".format('Spam & Eggs:', 6.99)
'        Spam & Eggs:   6.99'


>>> "Art: {a:5d},  Price: {p:8.2f}".format(a=453, p=59.058)
'Art:   453,  Price:    59.06'
>>> 


>>> price = 11.23
>>> f"Price in Euro: {price}"
'Price in Euro: 11.23'
>>> f"Price in Swiss Franks: {price * 1.086}"
'Price in Swiss Franks: 12.195780000000001'
>>> f"Price in Swiss Franks: {price * 1.086:5.2f}"
'Price in Swiss Franks: 12.20'
>>> for article in ["bread", "butter", "tea"]:
...     print(f"{article:>10}:")
... 
     bread:
    butter:
       tea:


https://xlsxwriter.readthedocs.io/workbook.html

#each web page has the following features for alice to choose from
ram_www_navigation.csv
RowNumber,domain,nextButton,previousButton,searchField,searchButton,link

#alice remembers the following information
ram_www_history.csv
RowNumber,dateFirstVisited,vistDate,domain,lastDiscoveryCount,totalDiscoveries,totalURLs,totalWords,totalPhrases,totalRequests,totalFailedRequests,progress,ahapRanking,rankCertainty

#alice discovers new web pages and prioritizes them
ram_www_future.csv
RowNumber,foundDate,initialDiscoveryDomain,uniqueReferenceCount,referenceAhapRanking,priority

#alice records every url from every domain
ram_url_links.csv
RowNumber,timestamp,domainSource,hyperlink,hyperlinkText,totalFrequency,uniqueDomainFrequency,ahapKeys,aliceKeys,good,neutral,negative

#alice records every english word from every domain
ram_nlp_words.csv
RowNumber,timestamp,domain,word,totalFrequency,uniqueDomainFrequency,ahapMatches,good,neutral,negative

#alice records every english phrase from every domain
ram_nlp_phrases.csv
RowNumber,timestamp,domain,phrase,totalFrequency,uniqueDomainFrequency,ahapKeys,aliceKeys,good,neutral,negative

#alice interprets natural language for each domain
RowNumber,domain,summary,classification

#alice generates a report for ahap
ram_ahap_report.xlsx
RowNumber,timestamp,name,summary,ranking,domain,email,title,address,phone,description,frequency,recency

190517












